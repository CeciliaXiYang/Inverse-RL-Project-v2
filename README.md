
## MaxEnt IRL
Ziebart et al. The classic Maximum Entropy Inverse RL:<br />
https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf

Wulfmeier et al. MaxEnt IRL with neural net reward function, known dynamics:<br />
http://www.robots.ox.ac.uk/~mobile/Papers/DeepIRL_2015.pdf<br />
Code: https://github.com/stormmax/irl-imitation <br />
      https://github.com/MatthewJA/Inverse-Reinforcement-Learning

Chelsea Finn, et al. ICML ’16 Sampling based method for MaxEnt IRL that handles unknown dynamics and deep reward functions:<br />
https://arxiv.org/pdf/1603.00448.pdf <br />
Code: https://github.com/justinjfu/inverse_rl

## Adversarial IRL
Ho & Ermon. NIPS ’16. Generative Adversarial Imitation Learning, Inverse RL method using generative adversarial networks: <br />
https://arxiv.org/pdf/1606.03476.pdf <br />
Code: https://github.com/andrewliao11/gail-tf

Baram et al. ICML ’17. use learned dynamics model to backdrop through discriminator: <br />
http://proceedings.mlr.press/v70/baram17a/baram17a.pdf <br />
Code: https://github.com/itaicaspi/mgail

## Bayesian IRL
Ramachandran & Amir IJCAI '07. Classic BIRL paper: <br />
https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf

Choi & Kim. NIPS '11. propose a gradient method to calculate the MAP estimate that is based on the (sub)differentiability of the posterior distribution: <br />
https://papers.nips.cc/paper/4479-map-inference-for-bayesian-inverse-reinforcement-learning.pdf

## Bayesian Nonparametric IRL
Choi & Kim. NIPS '12. use Dirichlet process mixture model: <br />
https://papers.nips.cc/paper/4737-nonparametric-bayesian-inverse-reinforcement-learning-for-multiple-reward-functions.pdf

## Thompson sampling-style approaches for RL
Osband et al. Deep Exploration via Randomized Value Functions:
https://arxiv.org/pdf/1703.07608.pdf

Osband et al. Posterior Sampling:
https://arxiv.org/pdf/1306.0940.pdf

Osband et al. Bootstrapped DQN: 
https://arxiv.org/pdf/1602.04621.pdf
